{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data using the Kaggle Overview as a guide for parsing. Then save into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('games.json', 'r', encoding='utf-8') as fin:\n",
    "    dataset = json.load(fin)\n",
    "\n",
    "# Create a list to store the flattened data\n",
    "games_data = []\n",
    "\n",
    "for app_id, game in dataset.items():\n",
    "    # Flatten nested lists into comma-separated strings\n",
    "    developers_str = ','.join(game['developers']) if game['developers'] else ''\n",
    "    publishers_str = ','.join(game['publishers']) if game['publishers'] else ''\n",
    "    categories_str = ','.join(game['categories']) if game['categories'] else ''\n",
    "    genres_str = ','.join(game['genres']) if game['genres'] else ''\n",
    "    tags_str = ','.join(str(tag) for tag in game['tags']) if game['tags'] else ''\n",
    "    screenshots_str = ','.join(game['screenshots']) if game['screenshots'] else ''\n",
    "    movies_str = ','.join(game['movies']) if game['movies'] else ''\n",
    "\n",
    "    # Create a flattened dictionary for this game\n",
    "    game_dict = {\n",
    "        'AppID': app_id,\n",
    "        'Name': game['name'],\n",
    "        'Release date': game['release_date'],\n",
    "        'Estimated owners': game['estimated_owners'],\n",
    "        'Peak CCU': game['peak_ccu'],\n",
    "        'Required age': game['required_age'],\n",
    "        'Price': game['price'],\n",
    "        'DiscountDLC count': game['dlc_count'],\n",
    "        'About the game': game['detailed_description'],\n",
    "        'Supported languages': game['supported_languages'],\n",
    "        'Full audio languages': game['full_audio_languages'],\n",
    "        'Reviews': game['reviews'],\n",
    "        'Header image': game['header_image'],\n",
    "        'Website': game['website'],\n",
    "        'Support url': game['support_url'],\n",
    "        'Support email': game['support_email'],\n",
    "        'Windows': game['windows'],\n",
    "        'Mac': game['mac'],\n",
    "        'Linux': game['linux'],\n",
    "        'Metacritic score': game['metacritic_score'],\n",
    "        'Metacritic url': game['metacritic_url'],\n",
    "        'User score': game['user_score'],\n",
    "        'Positive': game['positive'],\n",
    "        'Negative': game['negative'],\n",
    "        'Score rank': game['score_rank'],\n",
    "        'Achievements': game['achievements'],\n",
    "        'Recommendations': game['recommendations'],\n",
    "        'Notes': game['notes'],\n",
    "        'Average playtime forever': game['average_playtime_forever'],\n",
    "        'Average playtime two weeks': game['average_playtime_2weeks'],\n",
    "        'Median playtime forever': game['median_playtime_forever'],\n",
    "        'Median playtime two weeks': game['median_playtime_2weeks'],\n",
    "        'Developers': developers_str,\n",
    "        'Publishers': publishers_str,\n",
    "        'Categories': categories_str,\n",
    "        'Genres': genres_str,\n",
    "        'Tags': tags_str,\n",
    "        'Screenshots': screenshots_str,\n",
    "        'Movies': movies_str\n",
    "    }\n",
    "    games_data.append(game_dict)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(games_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some preprocessing of the text in the description column and save it into the processed_description column\n",
    "\n",
    "Preprocessing does:\n",
    "- convert to lower case\n",
    "- remove anything that is not a letter\n",
    "- remove extra whitespace\n",
    "- tokenize by splitting on whitespace\n",
    "- remove stopwords belonging to nltk.corpus.stopwords.words('english')\n",
    "- Lemmatize with WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ben/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ben/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ben/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ben/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing special characters and extra whitespace\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_description(text):\n",
    "    \"\"\"\n",
    "    Process game description by removing stopwords, lemmatizing, and tokenizing\n",
    "    \"\"\"\n",
    "    # Clean the text first\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Simple tokenization by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def process_game_data(df):\n",
    "    \"\"\"\n",
    "    Process the entire dataframe of game descriptions\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Process the \"About the game\" column\n",
    "    processed_df['processed_description'] = processed_df['About the game'].apply(\n",
    "        lambda x: process_description(str(x)) if pd.notnull(x) else ''\n",
    "    )\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_game_data(df) # takes about a minute and 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        galactic bowling exaggerated stylized bowling ...\n",
      "1        law look showdown atop train last fight good l...\n",
      "2        jolt project army new robotics project jolt co...\n",
      "3        henosis mysterious platform puzzler player pro...\n",
      "4        game play hacker arranged deal gangster protag...\n",
      "                               ...                        \n",
      "97405    femdom game world fascinating series video gam...\n",
      "97406    join discord server game enter charming world ...\n",
      "97407    mission brief deployed heavily guarded compoun...\n",
      "97408    welcome escape garage thrilling escape game se...\n",
      "97409    scan brain lobe organize amp clear dangerous n...\n",
      "Name: processed_description, Length: 97410, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_df['processed_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the index with more fields than we care for takes about 7 and a half minutes. Most of the processing time here is from storing the description I'm guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding docs to writer\n",
      "commiting to index\n"
     ]
    }
   ],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the schema for game data\n",
    "schema = Schema(\n",
    "    app_id=ID(stored=True),\n",
    "    name=TEXT(stored=True),\n",
    "    # description=TEXT(stored=True),\n",
    "    processed_description=TEXT(stored=True),\n",
    "    short_description=TEXT(stored=True),\n",
    "    developers=KEYWORD(stored=True, commas=True),\n",
    "    publishers=KEYWORD(stored=True, commas=True),\n",
    "    categories=KEYWORD(stored=True, commas=True),\n",
    "    genres=KEYWORD(stored=True, commas=True),\n",
    "    tags=KEYWORD(stored=True, commas=True),\n",
    "    price=NUMERIC(stored=True, numtype=float),\n",
    "    release_date=TEXT(stored=True),\n",
    "    languages=KEYWORD(stored=True, commas=True),\n",
    "    metacritic_score=NUMERIC(stored=True)\n",
    ")\n",
    "\n",
    "# Create index directory\n",
    "if os.path.exists(\"index\"):\n",
    "    shutil.rmtree(\"index\")  # Removes directory and all contents for overwriting previous index\n",
    "    \n",
    "os.mkdir(\"index\")\n",
    "\n",
    "# Create the index\n",
    "ix = create_in(\"index\", schema)\n",
    "writer = ix.writer()\n",
    "\n",
    "print(\"adding docs to writer\")\n",
    "# Add documents from your DataFrame\n",
    "for _, game in processed_df.iterrows():\n",
    "    writer.add_document(\n",
    "        app_id=str(game['AppID']),\n",
    "        name=game['Name'],\n",
    "        # description=game['About the game'],\n",
    "        processed_description=game['processed_description'],\n",
    "        developers=game['Developers'],\n",
    "        publishers=game['Publishers'],\n",
    "        categories=game['Categories'],\n",
    "        genres=game['Genres'],\n",
    "        tags=game['Tags'],\n",
    "        price=float(game['Price']),\n",
    "        release_date=game['Release date'],\n",
    "        languages=game['Supported languages'],\n",
    "        metacritic_score=int(game['Metacritic score'])\n",
    "    )\n",
    "\n",
    "print(\"commiting to index\")\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example query using Whoosh. I think a good next step is modifying the MyIndexReader from assignment three to get postings or something along those lines from the processed descriptions for terms. Then also want description length and collection length for performing dirichlet smoothing. For the powerpoint, it cold also be interesting to make some basic plots about the distribution of word frequencies, distribution of categories/genres etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example search usage:\n",
    "from whoosh.qparser import QueryParser\n",
    "# Takes about \n",
    "with ix.searcher() as searcher:\n",
    "    query = QueryParser(\"description\", ix.schema).parse(\"multiplayer action\")\n",
    "    results = searcher.search(query)\n",
    "    for r in results:\n",
    "        print(f\"{r['name']} - {r['app_id']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
